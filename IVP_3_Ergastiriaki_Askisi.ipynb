{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IVP_3_Ergastiriaki_Askisi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3η ομαδική εργασία στη Τεχνολογία και Ανάλυση Εικόνων και Βίντεο\n",
        "\n"
      ],
      "metadata": {
        "id": "4_Ya-bX5zpqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3η Εργαστηριακή Άσκηση\n",
        "\n",
        "### Δημήτρης Καλαθάς 03118016\n",
        "\n",
        "### Δημήτρης Μπακάλης 03118163\n",
        "\n",
        "### Ομάδα 7"
      ],
      "metadata": {
        "id": "cXCrSPO3zt7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Θεωρητικό Μέρος:"
      ],
      "metadata": {
        "id": "JUk3i7lJz4Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**α) Ποια είναι η διαφορά του bounding box και του anchor box στο YOLO? (Εξηγήστε συνοπτικά)**"
      ],
      "metadata": {
        "id": "L7OuGj26z9qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το bounding box με το anchor box διαφέρουν, κυρίως, ως προς το πλήθος των αντικειμένων που μπορούν να ανιχνέυσουν σε ένα grid cell. Πιο συγκεκριμένα, με το bounding ανιχνεύεται, μόνο, ένα αντικείμενο, ενώ με τα anchor boxes αρχικοποιούνται,τόσα όσα και οι κλάσεις που ανιχνεύουμε. Το σχήμα, με το οποίο αρχικοποιούνται προσεγγίζει, το αντίστοιχο σχήμα κάθε κλάσης."
      ],
      "metadata": {
        "id": "JE9KDu4K0ClV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**β) Ποιες θα είναι οι διαστάσεις του πίνακα εξόδου (πρόβλεψης) y_hat του αλγορίθμου YOLO θεωρώντας ότι έχουμε δυο anchor boxes και 3 κλάσεις? Αναφέρετε επίσης το ρόλο για κάθε στοιχείο του πίνακα αυτού.**"
      ],
      "metadata": {
        "id": "-lGSUeySMFg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δεδομένου ότι έχουμε δύο anchor boxes και 3 κλάσεις, ο πίνακας εξόδου θα είναι ένα διάνυσμα, το οποίο για κάθε anchor box θα περιέχει:\n",
        "\n",
        "*   την πιθανότητα pc, που είναι ενδεικτική της παρουσίας κάποιου αντικειμένου.\n",
        "\n",
        "*   4 παραμέτρους, που σηματοδοτούν τις συντεταγμένες του bounding box (συνήθως οι πρώτες δύο είναι το πάνω αριστερά σημείο και οι υπόλοιπες το πλάτος και το ύψος).\n",
        "\n",
        "*   3 παραμέτρους, οι οποίες κατατάσσουν το αντικείμενο στην αντίστοιχη κλάση.\n",
        "\n",
        "Επομένως, συνολικά έχουμε (1+4+3)x2 = 16 στοιχεία, που σημαίνει πως ο πίνακας εξόδου θα έχει διαστάσεις 16x1. "
      ],
      "metadata": {
        "id": "ApYJ2CMLMYPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**γ) Εξηγήστε συνοπτικά την έννοια της μεθόδου Non-max suppression. Σε τι χρησιμεύει?**"
      ],
      "metadata": {
        "id": "hAwZ-fBYMY2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η μέθοδος Non-max suppresion χρησιμοποιείται, για να δώσει λύση σε περιπτώσεις, όπου εξάγεται ένα πλήθος από bounding boxes για το ίδιο αντικείμενο. Με την εφαρμογή του συγκεκριμένου αλγορίθμου φιλτράρουμε αυτά τα σχήματα και κρατάμε το πιο αντιπροσωπευτικό. Αναλυτικότερα, πρόκειται για έναν επαναληπτικό αλγόριθμο, ο οποίος, αρχικά απορρίπτει όσα bounding boxes έχουν pc <= 0.6, και στη συνέχεια επαναλαμβάνει την ακόλουθη διαδικασία:\n",
        "\n",
        "*   Επιλέγει και εξάγει ως αποτέλεσμα το bounding box με το μεγαλύτερο pc. \n",
        "\n",
        "*   Απορρίπτει όσα bounding boxes έχουν IoU (Intersection over Union) μικρότερο από ένα κατώφλι (συνήθως 0.5) με το αποτέλεσμα του προηγούμενου βήματος."
      ],
      "metadata": {
        "id": "cqQ30EjUMZ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**δ) Στο αλγόριθμο SORT το ταίριασμα (matching) των αντικειμένων με ποια μέθοδο γίνεται? Αναφέρετε ένα παράδειγμα μετρικής για το bounding boxes distance.**"
      ],
      "metadata": {
        "id": "9TWP-oIdMjNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στον αλγόριθμο SORT (Simple Online Real-time Tracking), αφού εξαχθεί μία εκτίμηση των bounding boxes, μέσω του Detector, των θέσεων και της τροχιάς των αντικειμένων (Kalman filtering), το επόμενο βήμα είναι να γίνει το ταίριασμα των ανιχνευόμενων αντικειμένων. Αυτή η διαδικασία πραγματοποιείται με τον αλγόριθμο Hungarian, ο οποίος υπολογίζει το IoU, το οποίο είναι και η μετρική για bounding boxes distance, μεταξύ ενός Detected bounding box και όλων των υπολοίπων predicted bounding boxes. Προφανώς, εν τέλει ο αλγόριθμος επιστρέφει το bounding box με το μεγαλύτερο IoU."
      ],
      "metadata": {
        "id": "ShWIbiM0MjC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ε) Πώς ορίζεται η μέθοδος του tracking-by-detection και σε ποιο component αυτής της μεθόδου χρησιμοποιείται βαθιά μάθηση (deep learning)?**"
      ],
      "metadata": {
        "id": "7tRpXg5xMZT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η μέθοδος tracking-by-detection στηρίζεται στον ανέξαρτητο υπολογισμό των bounding boxes των αντικειμένων, που θέλουμε να ανιχνεύσουμε, σε κάθε frame ενός video. Αυτό έχει ως αποτέλεσμα, το πρόβλημα μας να μεταφέρεται στην αντιστοιχία των bounding boxes μεταξύ διαδοχικών frames, ώστε να βρεθεί το κατάλληλο ταίριασμα. Τεχνικές βαθιάς μάθησης, στη συγκεκριμένη μέθοδο, χρησιμοποιούνται στην υλοποίηση του αλγορίθμου, που χρησιμοποιείται για το detection. Πιο συγκεκριμένα, προτιμάται ο αλγόριθμος YOLOv3, ο οποίος βασίζεται στο μοντέλο βαθιάς μάθησης DarkNet, καθώς αποτελείται από από τα 53 προεκπαιδευμένα layers του, σε συνδυασμό με 53 νέα layers, που προστίθενται (συνολικά 106 layers)."
      ],
      "metadata": {
        "id": "addaeZz4Miac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Πειραματικό Μέρος:"
      ],
      "metadata": {
        "id": "_fVtz15lK6nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount to google drive.\n",
        "Το κάνουμε αυτό για έχουμε πρόσβαση στα python modules που χρειαζόμαστε."
      ],
      "metadata": {
        "id": "D4-ydCz6NZDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vWHTLQdvNZr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c45e29-52ae-4e25-8b36-88d014bdefd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για να μπορέσετε να έχετε πρόσβαση σε όλα τα αρχεία του φακέλου Lab_motion_tracking_exercise θα πρέπει πριν τρέξετε το παρακάτω κελί να πατε στο drive του εργαστηρίου, να κάνετε δεξί κλικ στο Lab_motion_tracking_exercise, να πατήσετε Add a shortcut to drive και μετά να πατήσετε My Drive."
      ],
      "metadata": {
        "id": "KyaGt3rVNo0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os \n",
        "sys.path.insert(0,'/content/drive/My Drive/Lab_motion_tracking_exercise') \n",
        "print(os.listdir('/content/drive/My Drive/Lab_motion_tracking_exercise')) # Έλεγχος ότι έχουμε όλα τα απαραίτητα αρχεία"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYLbA5GxNpUE",
        "outputId": "e9d2d73f-3284-4f01-a8fb-04f7b6847aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['config', 'images', 'utils', 'models.py', 'road_trafifc.mp4', 'sort.py', '__pycache__', 'Object_Detection_and_Tracking.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Φορτώνουμε τις απαραίτητες βιβλιοθήκες"
      ],
      "metadata": {
        "id": "7GRb2X4AN_Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models import *\n",
        "from utils import *\n",
        "\n",
        "import os, sys, time, datetime, random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "qYqNF0RMOANl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taWMHQlUODkK",
        "outputId": "0279c34f-a450-44ee-f626-211e6d188475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m pip install filterpy==1.4.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9PLvmYbJYg-",
        "outputId": "00cf4c99-ee09-4fa2-86bd-16a73283f457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting filterpy==1.4.5\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 25.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from filterpy==1.4.5) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from filterpy==1.4.5) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy==1.4.5) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy==1.4.5) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->filterpy==1.4.5) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy==1.4.5) (1.15.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=a27bfc0adb9fd25921b9ada2b4ac2cc67c228e0496418adedd2d5cea545198b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Φορτώνουμε τα προ-εκπαιδευμένα στο COCO dataset βάρη του μοντέλου YOLOv3\n",
        "Φορτώνουμε τις μονο τις κλάσεις που θέλουμε να κάνουμε detection (ανθρώπους, αυτοκίνητα κλπ) "
      ],
      "metadata": {
        "id": "hDTX6MgZOUip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path='./drive/My Drive/Lab_motion_tracking_exercise/config/yolov3.cfg'\n",
        "weights_path='./drive/My Drive/Lab_motion_tracking_exercise/config/yolov3.weights' \n",
        "class_path='./drive/My Drive/Lab_motion_tracking_exercise/config/coco.names'"
      ],
      "metadata": {
        "id": "LdoOJxKIOPAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size=416 #image size\n",
        "\n",
        "# Load model and weights\n",
        "model = Darknet(config_path, img_size=img_size)\n",
        "model.load_weights(weights_path)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "classes = utils.load_classes(class_path)\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ih0Sm9lPAv6",
        "outputId": "1cb1acaa-f0e9-435d-fa23-a0c0ce48b9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_image(img,conf_thres,nms_thres):\n",
        "    # scale and pad image\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "    # convert image to Tensor\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = Variable(image_tensor.type(Tensor))\n",
        "    # run inference on the model and get detections\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "    return detections[0]"
      ],
      "metadata": {
        "id": "NoDsduqDRozO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1ο Πείραμα** (conf_thres=0.05, nms_thres=0.4):"
      ],
      "metadata": {
        "id": "kd9DWxVHKb0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline \n",
        "import cv2\n",
        "from IPython.display import clear_output\n",
        "\n",
        "videopath = './drive/My Drive/7.mp4'  # Εδώ θα αλλάξετε το path για να πάρετε το video που αντιστοιχεί στην ομάδα σας\n",
        "                                                                      # Το video θα πρεπει αρχικά να το έχετε βάλει στο drive σας \n",
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "# initialize Sort object and video capture\n",
        "from sort import *\n",
        "vid = cv2.VideoCapture(videopath)\n",
        "mot_tracker = Sort() \n",
        "frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "#while(True):\n",
        "for ii in range(int(frames/10)):\n",
        "    ret, frame = vid.read()\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg,0.05,0.4)\n",
        "\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "    if detections is not None:\n",
        "        print(\"ii\", ii)\n",
        "        print(detections.shape)\n",
        "        tracked_objects = mot_tracker.update(detections.cpu())\n",
        "        print(tracked_objects.shape)\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            color = [i * 255 for i in color]\n",
        "            cls = classes[int(cls_pred)]\n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
        "            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
        "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "\n",
        "    fig=figure(figsize=(12, 8))\n",
        "    title(\"Video Stream\")\n",
        "    imshow(frame)\n",
        "    show()\n",
        "    #clear_output(wait=True)"
      ],
      "metadata": {
        "id": "J8Dm-kS7R4xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2ο Πείραμα** (conf_thres=0.95, nms_thres=0.4):"
      ],
      "metadata": {
        "id": "QDEpNdJeKzbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "# initialize Sort object and video capture\n",
        "vid = cv2.VideoCapture(videopath)\n",
        "mot_tracker = Sort() \n",
        "frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "#while(True):\n",
        "for ii in range(int(frames/10)):\n",
        "    ret, frame = vid.read()\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg,0.95,0.4)\n",
        "\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "    if detections is not None:\n",
        "        print(\"ii\", ii)\n",
        "        print(detections.shape)\n",
        "        tracked_objects = mot_tracker.update(detections.cpu())\n",
        "        print(tracked_objects.shape)\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            color = [i * 255 for i in color]\n",
        "            cls = classes[int(cls_pred)]\n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
        "            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
        "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "\n",
        "    fig=figure(figsize=(12, 8))\n",
        "    title(\"Video Stream\")\n",
        "    imshow(frame)\n",
        "    show()\n",
        "    #clear_output(wait=True)"
      ],
      "metadata": {
        "id": "98vlLzogK-eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3ο Πείραμα** (conf_thres=0.8, nms_thres=0.05):"
      ],
      "metadata": {
        "id": "xZWfaUh7QVNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "# initialize Sort object and video capture\n",
        "vid = cv2.VideoCapture(videopath)\n",
        "mot_tracker = Sort() \n",
        "frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "#while(True):\n",
        "for ii in range(int(frames/10)):\n",
        "    ret, frame = vid.read()\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg,0.8,0.05)\n",
        "\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "    if detections is not None:\n",
        "        print(\"ii\", ii)\n",
        "        print(detections.shape)\n",
        "        tracked_objects = mot_tracker.update(detections.cpu())\n",
        "        print(tracked_objects.shape)\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            color = [i * 255 for i in color]\n",
        "            cls = classes[int(cls_pred)]\n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
        "            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
        "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "\n",
        "    fig=figure(figsize=(12, 8))\n",
        "    title(\"Video Stream\")\n",
        "    imshow(frame)\n",
        "    show()\n",
        "    #clear_output(wait=True)"
      ],
      "metadata": {
        "id": "N8WDgMveQe34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4ο Πείραμα** (conf_thres=0.8, nms_thres=0.95):"
      ],
      "metadata": {
        "id": "Cp3bOGiGQjLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "# initialize Sort object and video capture\n",
        "vid = cv2.VideoCapture(videopath)\n",
        "mot_tracker = Sort() \n",
        "frames = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "#while(True):\n",
        "for ii in range(int(frames/10)):\n",
        "    ret, frame = vid.read()\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg,0.8,0.95)\n",
        "\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "    if detections is not None:\n",
        "        print(\"ii\", ii)\n",
        "        print(detections.shape)\n",
        "        tracked_objects = mot_tracker.update(detections.cpu())\n",
        "        print(tracked_objects.shape)\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            color = [i * 255 for i in color]\n",
        "            cls = classes[int(cls_pred)]\n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
        "            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
        "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "\n",
        "    fig=figure(figsize=(12, 8))\n",
        "    title(\"Video Stream\")\n",
        "    imshow(frame)\n",
        "    show()\n",
        "    #clear_output(wait=True)"
      ],
      "metadata": {
        "id": "39_aIgZkQmTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Παρατηρήσεις:**"
      ],
      "metadata": {
        "id": "SRV3twq0a95n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Σχετικά με το confidence threshold (conf_thres):**\n",
        "\n",
        "Αντιστοιχεί στην παράμετρο pc που περιγράψαμε στο θεωρητικό μέρος. Όπως φαίνεται και στα παραπάνω παραδείγματα, όσο αυξάνουμε το συγκεκριμένο κατώφλι, τόσο λιγότερα αντικείμενα αναγνωρίζονται στο detection, καθώς, προκειμένου να θεωρήσουμε ως αντικείμενο ένα μέρος του frame, πρέπει να έχει αρκετά μεγάλο pc (μεγαλύτερο του 0.95 στη δική μας περίπτωση). Αντίθετα, όσο μικρότερο το θέσουμε, τόσο περισσότερα αντικείμενα θα ανιχνευθούν με την ίδια λογική.\n",
        "\n",
        "**Σχετικά με το Non-max suppresion threshold (nms_thres):**\n",
        "\n",
        "Αντιστοιχεί στην αντεστραμμένη παράμετρο για το κατώφλι του IoU (Intersection over Union) για τον αλγόριθμο Non-max suppresion. Για αυτό τον λόγο, όσο αυξάνουμε την τιμή της, τόσο περισσότερα bounding boxes είναι πιθανό να κρατήσουμε για κάθε αντικείμενο που ανιχνεύεται. Αντίστοιχα, με τη μείωση της τιμής της κρατάμε, κατά πάσα πιθανότητα, το bounding box που προσεγγίζει καλύτερα το σχήμα του αντικειμένου."
      ],
      "metadata": {
        "id": "iINamTvZbC3x"
      }
    }
  ]
}